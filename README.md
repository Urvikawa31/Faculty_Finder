# Faculty Finder â€“ Data Engineering Pipeline

## Overview
**Faculty Finder** is an end-to-end **Data Engineering project** that builds a scalable pipeline to discover, extract, store, and serve faculty information from a university website. The system prepares high-quality data for downstream **semantic search and NLP-based applications**.

This repository focuses on **Phase 1: Data Engineering**, covering ingestion, storage, and serving layers using industry best practices.

---

## Problem Statement
Faculty expertise information is scattered across unstructured HTML pages and multiple directories. Traditional keyword-based search fails to identify relevant faculty members when exact terms are missing.

Before applying NLP or semantic search, a **robust, reproducible data pipeline** is required to extract and organize this information.

---

## Solution
This project implements a **production-style ETL pipeline** that:

- Crawls multiple faculty directories
- Extracts structured and semi-structured data from profile pages
- Stores **raw HTML content** for future processing
- Exposes data via **REST APIs** for downstream consumption

---

## Architecture

University Website (HTML)<br>
â†“<br>
Faculty URL Discovery<br>
â†“<br>
Profile-Level Scraping<br>
â†“<br>
SQLite Database (Raw HTML Stored)<br>
â†“<br>
FastAPI (Read-Only APIs)<br>

---

## ğŸ§© Pipeline Components

### 1ï¸ Ingestion
- Crawls multiple faculty directories:
  - Regular Faculty
  - Adjunct Faculty
  - International Adjunct Faculty
  - Distinguished Professors
  - Professors of Practice
- Dynamically discovers profile URLs
- Handles pagination, missing fields, and inconsistent layouts

---

### 2ï¸ Extraction
For each faculty profile, the pipeline extracts:
- Name, image, education
- Phone, address, email
- Biography
- Specialization
- Teaching
- Research
- Publications

**Raw HTML is preserved** to support later cleaning and NLP tasks.

---

### 3ï¸ Storage
- SQLite database for lightweight persistence
- Schema-driven design
- Raw HTML stored as TEXT fields (Bronze Layer)
- Safe re-runs using unique constraints

---

### 4ï¸ Serving (API Layer)
A **FastAPI-based read-only API** provides access to stored data:

| Endpoint | Description |
|--------|------------|
| `GET /faculty` | Fetch all faculty records |
| `GET /faculty/{id}` | Fetch faculty by ID |
| `GET /faculty/category/{category}` | Filter by faculty category |

Swagger UI available at:

###  Repository Structure

faculty-finder/<br>
â”‚<br>
â”œâ”€â”€ api/<br>
â”‚ â””â”€â”€ main.py<br>
â”‚<br>
â”œâ”€â”€ ingestion/<br>
â”‚ â”œâ”€â”€ discover_urls.py<br>
â”‚ â”œâ”€â”€ scrape_faculty.py<br>
â”‚ â””â”€â”€ pycache/<br>
â”‚<br>
â”œâ”€â”€ logs/<br>
â”‚ â”œâ”€â”€ llm_usage.md<br>
â”‚ â””â”€â”€ scraper.log<br>
â”‚<br>
â”œâ”€â”€ storage/<br>
â”‚ â”œâ”€â”€ db.py<br>
â”‚ â”œâ”€â”€ schema.sql<br>
â”‚ â”œâ”€â”€ init_db.py<br>
â”‚ â””â”€â”€ faculty.db<br>
â”‚<br>
â”œâ”€â”€ transformation/<br>
â”‚<br>
â”œâ”€â”€ run_pipeline.py<br>
â”œâ”€â”€ requirements.txt<br>
â””â”€â”€ README.md<br>

##  How to Run

### 1ï¸ Create Virtual Environment & Install Dependencies
```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2ï¸ Initialize Database
```bash
python storage/init_db.py
```

### 3ï¸ Run Data Pipeline
```bash
python run_pipeline.py
```

### 4ï¸ Start API Server
```bash
uvicorn api.main:app --reload
```
